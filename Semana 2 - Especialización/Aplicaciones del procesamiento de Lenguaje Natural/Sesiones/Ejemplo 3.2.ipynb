{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y fsspec datasets\n",
        "!pip install fsspec==2023.6.0 datasets --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XvlF_egbWuYq",
        "outputId": "960dddc4-48f8-4ce1-9f2e-d154974fa8b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: fsspec 2023.6.0\n",
            "Uninstalling fsspec-2023.6.0:\n",
            "  Successfully uninstalled fsspec-2023.6.0\n",
            "Found existing installation: datasets 3.6.0\n",
            "Uninstalling datasets-3.6.0:\n",
            "  Successfully uninstalled datasets-3.6.0\n",
            "Collecting fsspec==2023.6.0\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2023.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mJpwqgqdVX0J"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import random"
      ],
      "metadata": {
        "id": "OLTBtN69VhHy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfDqGGo2YtFl",
        "outputId": "b6283b1b-e8bd-4581-b6ab-924dba371feb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cargar todo el conjunto\n",
        "dataset_full = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "# 2. Seleccionar manualmente un subconjunto (por ejemplo, 2% ≈ 500 ejemplos)\n",
        "dataset = dataset_full.select(range(500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx9gPN_NVxof",
        "outputId": "658159a4-9724-4807-cee0-9d4631908f01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "counter = Counter()\n",
        "for ex in dataset:\n",
        "    counter.update(tokenize(ex[\"text\"]))\n",
        "\n",
        "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "for i, word in enumerate(counter.keys(), start=2):\n",
        "    vocab[word] = i\n",
        "inv_vocab = {i: w for w, i in vocab.items()}"
      ],
      "metadata": {
        "id": "3PjHGUoJW-f_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Codificar texto a índices\n",
        "def encode(text):\n",
        "    return [vocab.get(w, vocab[\"<unk>\"]) for w in tokenize(text)]\n",
        "\n",
        "# Crear secuencias fijas\n",
        "seq_len = 30\n",
        "inputs, targets = [], []\n",
        "\n",
        "for ex in dataset:\n",
        "    encoded = encode(ex[\"text\"])\n",
        "    for i in range(len(encoded) - seq_len):\n",
        "        seq = encoded[i:i+seq_len]\n",
        "        tgt = encoded[i+seq_len]\n",
        "        inputs.append(torch.tensor(seq))\n",
        "        targets.append(torch.tensor(tgt))"
      ],
      "metadata": {
        "id": "M9xQ3BJtXBy6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "dataset = IMDbDataset(inputs, targets)\n",
        "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "iL22mrKaXHHV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=128, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1])\n",
        "        return out"
      ],
      "metadata": {
        "id": "8dDoKiPxVfN5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WordLSTM(len(vocab)).to(device)"
      ],
      "metadata": {
        "id": "zgjzJ--0YzHJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "losses = []\n",
        "perplexities = []\n",
        "accuracies = []\n",
        "\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        output = model(batch_X)\n",
        "        loss = loss_fn(output, batch_y)\n",
        "\n",
        "        # Accuracy\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    losses.append(avg_loss)\n",
        "    perplexities.append(perplexity.item())\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f} | Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SFmDu-YXMvQ",
        "outputId": "95811436-9b7c-40cf-dacb-bd9f18495045"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 7.2301 | Perplexity: 1380.35 | Accuracy: 8.72%\n",
            "Epoch 2 | Loss: 6.0118 | Perplexity: 408.22 | Accuracy: 11.99%\n",
            "Epoch 3 | Loss: 4.9111 | Perplexity: 135.79 | Accuracy: 16.28%\n",
            "Epoch 4 | Loss: 4.0014 | Perplexity: 54.67 | Accuracy: 24.37%\n",
            "Epoch 5 | Loss: 3.3489 | Perplexity: 28.47 | Accuracy: 32.45%\n",
            "Epoch 6 | Loss: 2.8457 | Perplexity: 17.21 | Accuracy: 40.21%\n",
            "Epoch 7 | Loss: 2.4604 | Perplexity: 11.71 | Accuracy: 46.96%\n",
            "Epoch 8 | Loss: 2.1610 | Perplexity: 8.68 | Accuracy: 52.33%\n",
            "Epoch 9 | Loss: 1.9297 | Perplexity: 6.89 | Accuracy: 56.52%\n",
            "Epoch 10 | Loss: 1.7473 | Perplexity: 5.74 | Accuracy: 60.08%\n",
            "Epoch 11 | Loss: 1.5983 | Perplexity: 4.94 | Accuracy: 62.77%\n",
            "Epoch 12 | Loss: 1.4772 | Perplexity: 4.38 | Accuracy: 65.32%\n",
            "Epoch 13 | Loss: 1.3783 | Perplexity: 3.97 | Accuracy: 67.05%\n",
            "Epoch 14 | Loss: 1.3104 | Perplexity: 3.71 | Accuracy: 68.26%\n",
            "Epoch 15 | Loss: 1.2396 | Perplexity: 3.45 | Accuracy: 69.90%\n",
            "Epoch 16 | Loss: 1.1894 | Perplexity: 3.29 | Accuracy: 70.84%\n",
            "Epoch 17 | Loss: 1.1265 | Perplexity: 3.08 | Accuracy: 71.93%\n",
            "Epoch 18 | Loss: 1.0973 | Perplexity: 3.00 | Accuracy: 72.45%\n",
            "Epoch 19 | Loss: 1.0595 | Perplexity: 2.88 | Accuracy: 73.26%\n",
            "Epoch 20 | Loss: 1.0395 | Perplexity: 2.83 | Accuracy: 73.55%\n",
            "Epoch 21 | Loss: 1.0068 | Perplexity: 2.74 | Accuracy: 74.34%\n",
            "Epoch 22 | Loss: 0.9910 | Perplexity: 2.69 | Accuracy: 74.55%\n",
            "Epoch 23 | Loss: 0.9774 | Perplexity: 2.66 | Accuracy: 74.72%\n",
            "Epoch 24 | Loss: 0.9474 | Perplexity: 2.58 | Accuracy: 75.45%\n",
            "Epoch 25 | Loss: 0.9449 | Perplexity: 2.57 | Accuracy: 75.36%\n",
            "Epoch 26 | Loss: 0.9356 | Perplexity: 2.55 | Accuracy: 75.46%\n",
            "Epoch 27 | Loss: 0.9200 | Perplexity: 2.51 | Accuracy: 75.90%\n",
            "Epoch 28 | Loss: 0.9157 | Perplexity: 2.50 | Accuracy: 75.75%\n",
            "Epoch 29 | Loss: 0.9127 | Perplexity: 2.49 | Accuracy: 75.82%\n",
            "Epoch 30 | Loss: 0.9204 | Perplexity: 2.51 | Accuracy: 75.57%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, prompt, max_words=50):\n",
        "    model.eval()\n",
        "    tokens = encode(prompt)\n",
        "    input_seq = torch.tensor(tokens[-30:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated = tokens.copy()\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        out = model(input_seq)\n",
        "        next_token = torch.multinomial(torch.softmax(out[0], dim=0), 1).item()\n",
        "        generated.append(next_token)\n",
        "        input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[next_token]]).to(device)], dim=1)\n",
        "\n",
        "    return ' '.join(inv_vocab.get(t, \"<unk>\") for t in generated)\n"
      ],
      "metadata": {
        "id": "AhhGBUS7X8LY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, \"the movie was\", max_words=30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MarK23BNZLYa",
        "outputId": "ec29eed7-cb45-43bf-e26e-09c74376c62c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie was filmed, some fairly original place. there is another half way through the list, leave this one movie from the 1980s a pop soundtrack then what could even get off the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WordRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size=128, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, emb_size)\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1kmhQ34dbIOk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WordRNN(len(vocab)).to(device)"
      ],
      "metadata": {
        "id": "S9jXtKVIbLkF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "losses = []\n",
        "perplexities = []\n",
        "accuracies = []\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        output = model(batch_X)\n",
        "        loss = loss_fn(output, batch_y)\n",
        "\n",
        "        # Accuracy\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += (pred == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    losses.append(avg_loss)\n",
        "    perplexities.append(perplexity.item())\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f} | Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKZse8KebOR7",
        "outputId": "d2a2e85f-424b-4c0b-d89e-6829322afff9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 7.8027 | Perplexity: 2447.22 | Accuracy: 7.16%\n",
            "Epoch 2 | Loss: 6.6052 | Perplexity: 738.92 | Accuracy: 9.21%\n",
            "Epoch 3 | Loss: 5.8877 | Perplexity: 360.57 | Accuracy: 10.82%\n",
            "Epoch 4 | Loss: 6.0756 | Perplexity: 435.11 | Accuracy: 10.61%\n",
            "Epoch 5 | Loss: 6.5167 | Perplexity: 676.35 | Accuracy: 8.91%\n",
            "Epoch 6 | Loss: 6.4840 | Perplexity: 654.60 | Accuracy: 8.67%\n",
            "Epoch 7 | Loss: 6.7476 | Perplexity: 852.03 | Accuracy: 8.08%\n",
            "Epoch 8 | Loss: 6.2922 | Perplexity: 540.32 | Accuracy: 9.29%\n",
            "Epoch 9 | Loss: 5.9557 | Perplexity: 385.96 | Accuracy: 10.49%\n",
            "Epoch 10 | Loss: 6.0162 | Perplexity: 410.03 | Accuracy: 10.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, \"the movie was\", max_words=30))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esNTDUf-bgSN",
        "outputId": "97871a99-7d97-4d9b-b51c-a28b48a010ff"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the movie was there are attractive to watch a copy (for by alfredo inane.it well, holy cage are pure and they a little and stuff. tristan price (jesse metcalfe), balls schools parody hackneyed\n"
          ]
        }
      ]
    }
  ]
}