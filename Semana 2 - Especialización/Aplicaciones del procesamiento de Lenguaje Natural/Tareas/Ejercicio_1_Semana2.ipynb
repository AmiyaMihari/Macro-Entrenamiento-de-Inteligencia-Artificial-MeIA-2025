{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integrantes sala 7\n",
        "- Guillermo Jair Montiel Juárez\n",
        "- Juan Nicolás Pinilla Morales\n",
        "- Melesio Reyes Perez\n",
        "- Felipe Paez Gonzales\n",
        "- Daniel Caballero"
      ],
      "metadata": {
        "id": "gVL55JoRsgjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "!pip install fasttext\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz\n",
        "!gunzip cc.es.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S-3QpUjpsihg",
        "outputId": "f952cf5e-4a67-4e71-d337-8ba92ceb192d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313506 sha256=a168a6012d7229dcce6190e77f946519e83322bccf99dd7a5031d4e3083619a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "--2025-06-10 16:43:20--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.35.7.38, 13.35.7.50, 13.35.7.82, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.35.7.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4500107671 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.es.300.bin.gz’\n",
            "\n",
            "cc.es.300.bin.gz    100%[===================>]   4.19G   137MB/s    in 40s     \n",
            "\n",
            "2025-06-10 16:44:00 (109 MB/s) - ‘cc.es.300.bin.gz’ saved [4500107671/4500107671]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qUjmYNfasT21"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo de spaCy\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "1oZyhcPytjSt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Seleccionar dos textos cortos\n",
        "texto_a = \"La inteligencia artificial está revolucionando la medicina. Los algoritmos de IA pueden diagnosticar enfermedades con alta precisión. Esto transformará el futuro de la salud y el bienestar humano.\"\n",
        "texto_b = \"El aprendizaje automático es una rama de la inteligencia artificial. Se utiliza para crear modelos predictivos y mejorar la toma de decisiones. Su impacto en la industria tecnológica es innegable.\""
      ],
      "metadata": {
        "id": "wcSUwoMztojf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación del objeto doc (Preprocesa cada texto)\n",
        "doc_a = nlp(texto_a.lower())\n",
        "doc_b = nlp(texto_b.lower())"
      ],
      "metadata": {
        "id": "JfuLvP2Cyo-K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento\n",
        "def preprocesar_doc(doc):\n",
        "    \"\"\"\n",
        "    - Elimina signos de puntuación y stopwords\n",
        "    - Lematiza cada palabra\n",
        "    \"\"\"\n",
        "    return [\n",
        "        token.lemma_ for token in doc\n",
        "        if not token.is_punct and not token.is_stop\n",
        "    ]\n",
        "\n",
        "tokens_a = preprocesar_doc(doc_a)\n",
        "tokens_b = preprocesar_doc(doc_b)\n",
        "\n",
        "print(\"Tokens limpios A:\", tokens_a)\n",
        "print(\"Tokens limpios B:\", tokens_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vLqTcwGts5z",
        "outputId": "054fcc6b-4566-4e63-dee6-6ed708adf165"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens limpios A: ['inteligencia', 'artificial', 'revolucionar', 'medicina', 'algoritmo', 'ia', 'diagnosticar', 'enfermedad', 'alto', 'precisión', 'transformar', 'futuro', 'salud', 'bienestar', 'humano']\n",
            "Tokens limpios B: ['aprendizaje', 'automático', 'rama', 'inteligencia', 'artificial', 'utilizar', 'crear', 'modelo', 'predictivo', 'mejorar', 'toma', 'decisión', 'impacto', 'industria', 'tecnológico', 'innegable']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Análisis gramatical (POS tagging)\n",
        "def pos_frecuentes_doc(doc):\n",
        "    \"\"\"\n",
        "    - Obtiene la categoría gramatical (POS) de cada token\n",
        "    - Cuenta las etiquetas más frecuentes\n",
        "    \"\"\"\n",
        "    etiquetas = [token.pos_ for token in doc if not token.is_punct]\n",
        "    return Counter(etiquetas).most_common()\n",
        "\n",
        "print(\"\\nEtiquetas POS más frecuentes en A:\", pos_frecuentes_doc(doc_a))\n",
        "print(\"Etiquetas POS más frecuentes en B:\", pos_frecuentes_doc(doc_b))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54difU9xuWsH",
        "outputId": "0b3c5c42-063f-417c-c274-b3df16371764"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Etiquetas POS más frecuentes en A: [('NOUN', 9), ('DET', 6), ('ADJ', 3), ('VERB', 3), ('ADP', 3), ('AUX', 2), ('PRON', 1), ('CCONJ', 1)]\n",
            "Etiquetas POS más frecuentes en B: [('NOUN', 8), ('DET', 6), ('ADJ', 5), ('ADP', 4), ('VERB', 3), ('AUX', 2), ('PRON', 1), ('CCONJ', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reflexion: ¿Qué categorías predominan en cada uno?\n",
        "En ambos textos, predominan los sujetos (nouns), y esto coincide con el texto, debido a que en ambos se del sujeto como tema central de la oración (en este caso específico, sobre la inteligencia artificial)."
      ],
      "metadata": {
        "id": "urqXLnXSxhYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización con TF-IDF\n",
        "\n",
        "# Creamos el corpus con ambos textos\n",
        "corpus = [texto_a, texto_b]\n",
        "# Usamos las stopwords de spaCy para español\n",
        "vectorizer = TfidfVectorizer(stop_words=list(nlp.Defaults.stop_words))\n",
        "# Calculamos la matriz TF-IDF\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "O7m9pcX6uds0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similitud coseno\n",
        "# Calculamos la similitud del coseno entre los dos textos\n",
        "similitud = cosine_similarity(X[0], X[1])[0][0]\n",
        "print(f\"\\nSimilitud coseno (TF-IDF): {similitud:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXFs94nzvEdQ",
        "outputId": "89bfa359-01c6-403c-cd97-d3efa94c39b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similitud coseno (TF-IDF): 0.070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorización con embeddings de spaCy\n",
        "vector_a = nlp(texto_a).vector\n",
        "vector_b = nlp(texto_b).vector"
      ],
      "metadata": {
        "id": "vikcKzV9vFzs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similitud_emb = cosine_similarity([vector_a], [vector_b])[0][0]\n",
        "print(f\"Similitud coseno (spaCy embeddings): {similitud_emb:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya5Pyhy0uA-f",
        "outputId": "2918c509-b7fd-4ff5-e704-7e6d43e02961"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similitud coseno (spaCy embeddings): 0.817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretación de resultados\n",
        "\n",
        "## 1. ¿Los textos se parecen?\n",
        "Sí, en este caso particular, los textos son muy similares, porque hablan sobre la Inteligencia Artificial o el Aprendizaje Automático, y es tema central en ambos, tratándolo como un sujeto. Por ello es que usa muchos adjetivos y determinantes, que complementan las características de este sujeto. Además, la similitud del coseno coincide con lo previsto, pues es muy cercana al 1.\n",
        "\n",
        "## 2. ¿Qué influye más en la similitud: el tema, el vocabulario o la estructura?\n",
        "Para este método de vectorización en particular, se considera más el vocabulario a la hora hacer los embeddings. Si bien, la estructura completa es la que le da contexto a las palabras, la lemanización permite reducir las palabras a su mínimo estado base, perdiendo parte de su estructura en el proceso, para después vectorizar las palabras y tomar el promedio de los embeddings que conforman el texto."
      ],
      "metadata": {
        "id": "pDm0lLccyTf9"
      }
    }
  ]
}