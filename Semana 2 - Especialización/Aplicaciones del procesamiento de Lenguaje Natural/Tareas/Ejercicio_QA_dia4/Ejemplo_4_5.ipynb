{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integrantes Sala 7\n",
        "- Guillermo Jair Montiel Juárez\n",
        "- Juan Nicolás Pinilla Morales\n",
        "- Melesio Reyes Perez\n",
        "- Daniel Herrera Caballero\n",
        "- Juan David de la Cruz Martinez"
      ],
      "metadata": {
        "id": "9jY7nCvQu2G2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers --quiet"
      ],
      "metadata": {
        "id": "7HHlnj9Yq6YT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "5w0-M0rHq7wp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"NchuNLP/Chinese-Question-Answering\",\n",
        "    tokenizer=\"NchuNLP/Chinese-Question-Answering\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc7znWNzq9x5",
        "outputId": "f0becaed-b137-4b4d-952f-02b197052362"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Longformer acepta hasta ~4000 tokens, así que dividiremos el texto si es más largo\n",
        "def dividir_texto(texto, max_longitud=3900):\n",
        "    oraciones = texto.split(\". \")\n",
        "    bloques = []\n",
        "    actual = \"\"\n",
        "    for oracion in oraciones:\n",
        "        if len(actual) + len(oracion) < max_longitud:\n",
        "            actual += oracion + \". \"\n",
        "        else:\n",
        "            bloques.append(actual.strip())\n",
        "            actual = oracion + \". \"\n",
        "    if actual:\n",
        "        bloques.append(actual.strip())\n",
        "    return bloques\n"
      ],
      "metadata": {
        "id": "mnOJyc98s2e6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_largo(pregunta, archivo):\n",
        "    try:\n",
        "        if archivo is None:\n",
        "            return \"Por favor sube un archivo .txt con contenido válido.\"\n",
        "\n",
        "        # Leer archivo según tipo\n",
        "        if hasattr(archivo, \"name\"):\n",
        "            with open(archivo.name, encoding=\"utf-8\") as f:\n",
        "                contexto = f.read()\n",
        "        else:\n",
        "            return \"⚠️ No se pudo leer el archivo.\"\n",
        "\n",
        "        if not pregunta.strip():\n",
        "            return \"Escribe una pregunta válida.\"\n",
        "\n",
        "        bloques = [contexto] if len(contexto) < 3000 else dividir_texto(contexto)\n",
        "        respuestas = []\n",
        "\n",
        "        for bloque in bloques:\n",
        "            resultado = qa_pipeline(question=pregunta, context=bloque, top_k=3)\n",
        "            # Aseguramos que sea una lista (algunos modelos devuelven dict si top_k=1)\n",
        "            if isinstance(resultado, dict):\n",
        "                resultado = [resultado]\n",
        "            for r in resultado:\n",
        "                if r['score'] >= 0.001:  # solo respuestas confiables\n",
        "                    respuestas.append((r['answer'], round(r['score'], 3)))\n",
        "\n",
        "        if not respuestas:\n",
        "            return \"⚠️ No se encontró una respuesta con confianza suficiente.\"\n",
        "\n",
        "        # Ordenar por score descendente y mostrar top 3\n",
        "        respuestas.sort(key=lambda x: x[1], reverse=True)\n",
        "        texto_respuestas = \"\\n\".join(\n",
        "            [f\"• {resp} (confianza: {score})\" for resp, score in respuestas[:3]]\n",
        "        )\n",
        "        return texto_respuestas\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Error: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "ledoxEk6q_84"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HA6hD-mqq2IE"
      },
      "outputs": [],
      "source": [
        "# Interfaz Gradio\n",
        "iface = gr.Interface(\n",
        "    fn=responder_largo,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Pregunta\"),\n",
        "        gr.File(label=\"Carga un archivo .txt\", file_types=[\".txt\"])\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"QA con Longformer para documentos largos\",\n",
        "    description=\"Este sistema utiliza Longformer para responder preguntas en archivos largos (.txt). Divide automáticamente el contenido si supera los 4000 tokens.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "0eHMufukrCvQ",
        "outputId": "c7a4f0ef-fae9-4c34-c28c-5c229478ed0f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b74b77b23d194a54d8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b74b77b23d194a54d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}